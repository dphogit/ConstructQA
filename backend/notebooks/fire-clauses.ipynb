{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Fire Clauses\n",
    "\n",
    "This notebook demonstrates the QA pipeline using the fire clauses from `fire-clauses.json`.\n",
    "It shows how to:\n",
    "1. Create embeddings using `sentence-transformers` and save them to a numpy file\n",
    "2. Perform a manual test query (rather than using Qdrant) to simulate the semantic search process\n",
    "3. Performs extraction of answers from the closest matches using a BERT model from `transformers`"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Creating the embeddings"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[1], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msentence_transformers\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m SentenceTransformer\n\u001B[0;32m      2\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mnumpy\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mnp\u001B[39;00m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mpandas\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mpd\u001B[39;00m\n",
      "File \u001B[1;32mC:\\Uni\\SE700 - Research Project\\ConstructQA\\backend\\venv\\lib\\site-packages\\sentence_transformers\\__init__.py:3\u001B[0m\n\u001B[0;32m      1\u001B[0m __version__ \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m2.2.2\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m      2\u001B[0m __MODEL_HUB_ORGANIZATION__ \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124msentence-transformers\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m----> 3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdatasets\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m SentencesDataset, ParallelSentencesDataset\n\u001B[0;32m      4\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mLoggingHandler\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m LoggingHandler\n\u001B[0;32m      5\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mSentenceTransformer\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m SentenceTransformer\n",
      "File \u001B[1;32mC:\\Uni\\SE700 - Research Project\\ConstructQA\\backend\\venv\\lib\\site-packages\\sentence_transformers\\datasets\\__init__.py:1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mDenoisingAutoEncoderDataset\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m DenoisingAutoEncoderDataset\n\u001B[0;32m      2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mNoDuplicatesDataLoader\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m NoDuplicatesDataLoader\n\u001B[0;32m      3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mParallelSentencesDataset\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m ParallelSentencesDataset\n",
      "File \u001B[1;32mC:\\Uni\\SE700 - Research Project\\ConstructQA\\backend\\venv\\lib\\site-packages\\sentence_transformers\\datasets\\DenoisingAutoEncoderDataset.py:1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdata\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Dataset\n\u001B[0;32m      2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtyping\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m List\n\u001B[0;32m      3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mreaders\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mInputExample\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m InputExample\n",
      "File \u001B[1;32mC:\\Uni\\SE700 - Research Project\\ConstructQA\\backend\\venv\\lib\\site-packages\\torch\\__init__.py:1465\u001B[0m\n\u001B[0;32m   1463\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m library\n\u001B[0;32m   1464\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m TYPE_CHECKING:\n\u001B[1;32m-> 1465\u001B[0m     \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m _meta_registrations\n\u001B[0;32m   1467\u001B[0m \u001B[38;5;66;03m# Enable CUDA Sanitizer\u001B[39;00m\n\u001B[0;32m   1468\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mTORCH_CUDA_SANITIZER\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;129;01min\u001B[39;00m os\u001B[38;5;241m.\u001B[39menviron:\n",
      "File \u001B[1;32mC:\\Uni\\SE700 - Research Project\\ConstructQA\\backend\\venv\\lib\\site-packages\\torch\\_meta_registrations.py:7\u001B[0m\n\u001B[0;32m      5\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_prims_common\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mutils\u001B[39;00m\n\u001B[0;32m      6\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Tensor\n\u001B[1;32m----> 7\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_decomp\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m _add_op_to_registry, global_decomposition_table, meta_table\n\u001B[0;32m      8\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_ops\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m OpOverload\n\u001B[0;32m      9\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_prims\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m _elementwise_meta, ELEMENTWISE_PRIM_TYPE_PROMOTION_KIND\n",
      "File \u001B[1;32mC:\\Uni\\SE700 - Research Project\\ConstructQA\\backend\\venv\\lib\\site-packages\\torch\\_decomp\\__init__.py:170\u001B[0m\n\u001B[0;32m    168\u001B[0m \u001B[38;5;66;03m# populate the table\u001B[39;00m\n\u001B[0;32m    169\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_decomp\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdecompositions\u001B[39;00m\n\u001B[1;32m--> 170\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_refs\u001B[39;00m\n\u001B[0;32m    172\u001B[0m \u001B[38;5;66;03m# This list was copied from torch/_inductor/decomposition.py\u001B[39;00m\n\u001B[0;32m    173\u001B[0m \u001B[38;5;66;03m# excluding decompositions that results in prim ops\u001B[39;00m\n\u001B[0;32m    174\u001B[0m \u001B[38;5;66;03m# Resulting opset of decomposition is core aten ops\u001B[39;00m\n\u001B[0;32m    175\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcore_aten_decompositions\u001B[39m() \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Dict[OpOverload, Callable]:\n",
      "File \u001B[1;32m<frozen importlib._bootstrap>:1007\u001B[0m, in \u001B[0;36m_find_and_load\u001B[1;34m(name, import_)\u001B[0m\n",
      "File \u001B[1;32m<frozen importlib._bootstrap>:982\u001B[0m, in \u001B[0;36m_find_and_load_unlocked\u001B[1;34m(name, import_)\u001B[0m\n",
      "File \u001B[1;32m<frozen importlib._bootstrap>:925\u001B[0m, in \u001B[0;36m_find_spec\u001B[1;34m(name, path, target)\u001B[0m\n",
      "File \u001B[1;32m<frozen importlib._bootstrap_external>:1349\u001B[0m, in \u001B[0;36mfind_spec\u001B[1;34m(cls, fullname, path, target)\u001B[0m\n",
      "File \u001B[1;32m<frozen importlib._bootstrap_external>:1321\u001B[0m, in \u001B[0;36m_get_spec\u001B[1;34m(cls, fullname, path, target)\u001B[0m\n",
      "File \u001B[1;32m<frozen importlib._bootstrap_external>:1446\u001B[0m, in \u001B[0;36mfind_spec\u001B[1;34m(self, fullname, target)\u001B[0m\n",
      "File \u001B[1;32m<frozen importlib._bootstrap_external>:87\u001B[0m, in \u001B[0;36m_path_stat\u001B[1;34m(path)\u001B[0m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import os"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-11T04:57:57.077121200Z",
     "start_time": "2023-07-11T04:57:52.530545600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "DATA_DIR = os.path.join(os.getcwd(), '..', 'data')\n",
    "DATA_DIR"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Read the fire clauses into a dataframe\n",
    "path = os.path.join(DATA_DIR, 'fire-clauses.json')\n",
    "df = pd.read_json(path)\n",
    "\n",
    "# Because the clause is unique, we can use it as the index\n",
    "# Handle limitOnApplication NaNs by replacing with empty string\n",
    "df.set_index('clause', inplace=True)\n",
    "df['limitOnApplication'].fillna('', inplace=True)\n",
    "\n",
    "# First 10 clauses\n",
    "df.head(10)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load in the sentence transformer model - have a look and the comparisons here:\n",
    "# https://www.sbert.net/docs/pretrained_models.html#sentence-embedding-models/\n",
    "# multi-qa-MiniLM-L6-cos-v1 is trained for QA and is smaller with very minor loss in performance\n",
    "\n",
    "model = SentenceTransformer('multi-qa-MiniLM-L6-cos-v1')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Encode the clause contents to create sentence vector embeddings (combine `content` and `limitOnApplication`)\n",
    "\n",
    "sentences = (df['content'] + ' ' + df['limitOnApplication']).tolist()\n",
    "vectors = model.encode(sentences, show_progress_bar=True)\n",
    "\n",
    "# Expect a mxn matrix where m is the number of clauses and n is the embedding dimension of the model\n",
    "vectors.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Save the vectors to a numpy file for the script to load and insert into Qdrant\n",
    "\n",
    "save_path = os.path.join(DATA_DIR, 'fire-clauses.npy')\n",
    "np.save(save_path, vectors, allow_pickle=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Manual Test Query and Semantic Search\n",
    "\n",
    "Make sure that our vectors have been converted expectedly where we manually search for a clause and find the closest match (we don't use Qdrant here yet)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sentence_transformers import util"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Target clause C3.8\n",
    "question = 'How high must the smoke be above the floor when firefighters put out a fire with water?'\n",
    "context = df[df.index == 'C3.8']['content'].values[0]\n",
    "\n",
    "print(f'Question: {question}')\n",
    "print()\n",
    "print(f'Expected context: {context}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Encode the question\n",
    "question_vector = model.encode(question)\n",
    "question_vector.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Look for the top 3 closest matches - we use cosine similarity and gain all the scores in asc order.\n",
    "# With the sorted scores we get the last 3 (top 3) and then flip for descending order.\n",
    "# We then obtain from the data frame the clauses that match the top 3 scores\n",
    "\n",
    "scores = util.cos_sim(np.array([question_vector]), vectors)[0]\n",
    "top_score_ids = np.argsort(scores)[-3:].flip(0)\n",
    "top_scored_rows = df.iloc[top_score_ids][['content']]\n",
    "top_scored_rows"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "As we can see clause C3.8 was the top match.In the actual application, these embeddings need to be persisted in an actual vector database which we use Qdrant for. Qdrant has a client library to perform semantic search and have a nicer developer experience."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Answer Extraction with BERT\n",
    "\n",
    "From our top matches, we will attempt to extract the answer from the context using a BERT model."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load the model and tokenizer\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForQuestionAnswering\n",
    "\n",
    "model_name = 'deepset/tinyroberta-squad2'\n",
    "\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Method 1: Using the `pipeline` from `transformers` for automatic inference\n",
    "\n",
    "This is a more beginner-friendly approach, providing a general abstraction and allows you to use any of the pre-trained models from `transformers` to complete any inference task. We also show different ways to obtain answers from a varying number of contexts which could be used for experiments."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "qa_pipeline = pipeline('question-answering', model=model, tokenizer=tokenizer)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Result from top single context\n",
    "qa_pipeline(question=question, context=top_scored_rows['content'][0])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Concatenate all the top contexts and get the answer from the concatenated context\n",
    "qa_pipeline(question=question, context=''.join(top_scored_rows['content'].tolist()))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Individual answers from each context and get/sort by their probability scores\n",
    "qa_pipeline(question=[question] * len(top_scored_rows), context=top_scored_rows['content'].tolist())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Method 2: Applying the model directly\n",
    "\n",
    "This approach allows for more control over the inference process. This approach allows you to perform further research and analysis on the model's (intermediate) outputs and integrations for custom workflows."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Encode using the tokenizer\n",
    "inputs = tokenizer(question, top_scored_rows['content'][0], return_tensors='pt')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# no_grad() to disable gradient calculation as we are only performing inference and don't need back propagation\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "# Get the tokens of the predicted answer\n",
    "start_idx, end_idx = outputs.start_logits.argmax(), outputs.end_logits.argmax()\n",
    "predicted_answer_tokens = inputs['input_ids'][0, start_idx:end_idx + 1]\n",
    "predicted_answer_tokens"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Decode the tokens back to words to obtain the answer (trim string)\n",
    "tokenizer.decode(predicted_answer_tokens, skip_special_tokens=True).strip()"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Fire Clauses\n",
    "\n",
    "This notebook demonstrates the QA pipeline using the fire clauses from `fire-clauses.json`.\n",
    "It shows how to:\n",
    "1. Create embeddings using `sentence-transformers` and save them to a numpy file\n",
    "2. Perform a manual test query (rather than using Qdrant) to simulate the semantic search process\n",
    "3. Performs extraction of answers from the closest matches using a BERT model from `transformers`"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Creating the embeddings"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import os"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "DATA_DIR = os.path.join(os.getcwd(), '..', 'data')\n",
    "DATA_DIR"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Read the fire clauses into a dataframe\n",
    "path = os.path.join(DATA_DIR, 'fire-clauses.json')\n",
    "df = pd.read_json(path)\n",
    "\n",
    "# Because the clause is unique, we can use it as the index\n",
    "# Handle limitOnApplication NaNs by replacing with empty string\n",
    "df.set_index('clause', inplace=True)\n",
    "df['limitOnApplication'].fillna('', inplace=True)\n",
    "\n",
    "# First 10 clauses\n",
    "df.head(10)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load in the sentence transformer model - have a look and the comparisons here:\n",
    "# https://www.sbert.net/docs/pretrained_models.html#sentence-embedding-models/\n",
    "# multi-qa-MiniLM-L6-cos-v1 is trained for QA and is smaller with very minor loss in performance\n",
    "\n",
    "model = SentenceTransformer('multi-qa-MiniLM-L6-cos-v1')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Encode the clause contents to create sentence vector embeddings (combine `content` and `limitOnApplication`)\n",
    "\n",
    "sentences = (df['content'] + ' ' + df['limitOnApplication']).tolist()\n",
    "vectors = model.encode(sentences, show_progress_bar=True)\n",
    "\n",
    "# Expect a mxn matrix where m is the number of clauses and n is the embedding dimension of the model\n",
    "vectors.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Save the vectors to a numpy file for the script to load and insert into Qdrant\n",
    "\n",
    "save_path = os.path.join(DATA_DIR, 'fire-clauses.npy')\n",
    "np.save(save_path, vectors, allow_pickle=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Manual Test Query and Semantic Search\n",
    "\n",
    "Make sure that our vectors have been converted expectedly where we manually search for a clause and find the closest match (we don't use Qdrant here yet)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sentence_transformers import util"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Target clause C3.8\n",
    "question = 'How high must the smoke be above the floor when firefighters put out a fire with water?'\n",
    "context = df[df.index == 'C3.8']['content'].values[0]\n",
    "\n",
    "print(f'Question: {question}')\n",
    "print()\n",
    "print(f'Expected context: {context}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Encode the question\n",
    "question_vector = model.encode(question)\n",
    "question_vector.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Look for the top 3 closest matches - we use cosine similarity and gain all the scores in asc order.\n",
    "# With the sorted scores we get the last 3 (top 3) and then flip for descending order.\n",
    "# We then obtain from the data frame the clauses that match the top 3 scores\n",
    "\n",
    "scores = util.cos_sim(np.array([question_vector]), vectors)[0]\n",
    "top_score_ids = np.argsort(scores)[-3:].flip(0)\n",
    "top_scored_rows = df.iloc[top_score_ids][['content']]\n",
    "top_scored_rows"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "As we can see clause C3.8 was the top match.In the actual application, these embeddings need to be persisted in an actual vector database which we use Qdrant for. Qdrant has a client library to perform semantic search and have a nicer developer experience."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Answer Extraction with BERT\n",
    "\n",
    "From our top matches, we will attempt to extract the answer from the context using a BERT model."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load the model and tokenizer\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForQuestionAnswering\n",
    "\n",
    "model_name = 'deepset/tinyroberta-squad2'\n",
    "\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Method 1: Using the `pipeline` from `transformers` for automatic inference\n",
    "\n",
    "This is a more beginner-friendly approach, providing a general abstraction and allows you to use any of the pre-trained models from `transformers` to complete any inference task. We also show different ways to obtain answers from a varying number of contexts which could be used for experiments."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "qa_pipeline = pipeline('question-answering', model=model, tokenizer=tokenizer)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Result from top single context\n",
    "qa_pipeline(question=question, context=top_scored_rows['content'][0])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Concatenate all the top contexts and get the answer from the concatenated context\n",
    "qa_pipeline(question=question, context=''.join(top_scored_rows['content'].tolist()))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Individual answers from each context and get/sort by their probability scores\n",
    "qa_pipeline(question=[question] * len(top_scored_rows), context=top_scored_rows['content'].tolist())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Method 2: Applying the model directly\n",
    "\n",
    "This approach allows for more control over the inference process. This approach allows you to perform further research and analysis on the model's (intermediate) outputs and integrations for custom workflows."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Encode using the tokenizer\n",
    "inputs = tokenizer(question, top_scored_rows['content'][0], return_tensors='pt')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# no_grad() to disable gradient calculation as we are only performing inference and don't need back propagation\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "# Get the tokens of the predicted answer\n",
    "start_idx, end_idx = outputs.start_logits.argmax(), outputs.end_logits.argmax()\n",
    "predicted_answer_tokens = inputs['input_ids'][0, start_idx:end_idx + 1]\n",
    "predicted_answer_tokens"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Decode the tokens back to words to obtain the answer (trim string)\n",
    "tokenizer.decode(predicted_answer_tokens, skip_special_tokens=True).strip()"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
